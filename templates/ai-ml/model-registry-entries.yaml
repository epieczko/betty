# Model Registry Entry - Structured Format
# Platform: MLflow | SageMaker | Azure ML | Vertex AI | Databricks Unity Catalog

modelRegistryEntry:
  # ===========================================================================
  # BASIC MODEL INFORMATION
  # ===========================================================================
  basicInfo:
    modelName: "[model-name]"
    modelId: "[unique-identifier-in-registry]"
    version: "1.0.0"  # Semantic versioning: MAJOR.MINOR.PATCH
    description: "[Brief description of model purpose and use case]"

    registryPlatform: "[mlflow | sagemaker | azureml | vertexai | unity_catalog]"
    registryUrl: "https://mlflow.example.com/models/model-name"

    currentStage: "[development | staging | production | archived]"
    createdAt: "2025-01-15T10:30:00Z"
    lastModifiedAt: "2025-01-20T14:45:00Z"

  # ===========================================================================
  # MODEL METADATA
  # ===========================================================================
  metadata:
    tags:
      risk_tier: "[high | medium | low]"
      business_unit: "[unit-name]"
      use_case: "[fraud-detection | recommendation | classification | etc]"
      owner: "[team-name or email]"
      compliance_status: "[compliant | pending_review | needs_updates]"
      fairness_tested: "[true | false]"
      bias_assessed: "[true | false]"
      explainability: "[shap | lime | inherent | none]"
      model_type: "[classification | regression | clustering | generative]"

    customTags:
      - key: "project"
        value: "[project-name]"
      - key: "cost_center"
        value: "[cost-center-id]"
      - key: "sensitivity"
        value: "[high | medium | low]"

  # ===========================================================================
  # MODEL LINEAGE & PROVENANCE
  # ===========================================================================
  lineage:
    trainingRun:
      runId: "[mlflow-run-id or experiment-id]"
      experimentId: "[mlflow-experiment-id]"
      experimentName: "[experiment-name]"
      runUrl: "https://mlflow.example.com/experiments/123/runs/abc"

    trainingData:
      datasetName: "[dataset-name]"
      datasetVersion: "2.1.0"
      datasetLocation: "s3://bucket/datasets/dataset-name-v2.1.0"
      datasetChecksum: "sha256:abc123..."
      recordCount: 1000000
      featureCount: 125
      dataCollectionPeriod:
        start: "2023-01-01"
        end: "2024-12-31"

    codeVersion:
      repository: "https://github.com/org/repo"
      commitSha: "abc123def456..."
      branch: "main"
      tag: "v1.0.0"

    framework:
      name: "[tensorflow | pytorch | scikit-learn | xgboost | huggingface]"
      version: "2.15.0"

    dependencies:
      python: "3.11.5"
      packages:
        - name: "numpy"
          version: "1.26.0"
        - name: "pandas"
          version: "2.1.0"
        - name: "scikit-learn"
          version: "1.3.2"

    environment:
      condaEnv: "s3://bucket/artifacts/conda.yaml"
      requirementsTxt: "s3://bucket/artifacts/requirements.txt"
      dockerfile: "s3://bucket/artifacts/Dockerfile"

  # ===========================================================================
  # MODEL ARTIFACTS
  # ===========================================================================
  artifacts:
    modelFiles:
      location: "s3://bucket/models/model-name/v1.0.0"
      format: "[pickle | onnx | savedmodel | torchscript | huggingface]"
      size: "350MB"
      checksum: "sha256:def789..."

    serialization:
      primaryFormat: "onnx"
      alternativeFormats:
        - "tensorflow_savedmodel"
        - "torchscript"

    components:
      - name: "preprocessor"
        type: "sklearn.preprocessing.StandardScaler"
        location: "s3://bucket/models/model-name/v1.0.0/preprocessor.pkl"
      - name: "model"
        type: "xgboost.XGBClassifier"
        location: "s3://bucket/models/model-name/v1.0.0/model.json"
      - name: "postprocessor"
        type: "custom.ThresholdCalibrator"
        location: "s3://bucket/models/model-name/v1.0.0/postprocessor.pkl"

  # ===========================================================================
  # MODEL SIGNATURE (Input/Output Schema)
  # ===========================================================================
  signature:
    inputs:
      - name: "user_id"
        type: "int64"
        required: true
        description: "Unique user identifier"
      - name: "transaction_amount"
        type: "float64"
        required: true
        description: "Transaction amount in USD"
        min: 0.01
        max: 100000.0
      - name: "merchant_category"
        type: "string"
        required: true
        description: "Merchant category code"
        enum: ["retail", "food", "travel", "other"]
      - name: "features"
        type: "tensor"
        shape: [null, 125]  # [batch_size, feature_count]
        dtype: "float32"

    outputs:
      - name: "prediction"
        type: "int64"
        description: "Predicted class (0=benign, 1=fraud)"
        enum: [0, 1]
      - name: "probability"
        type: "float64"
        description: "Probability of positive class"
        min: 0.0
        max: 1.0
      - name: "confidence_score"
        type: "float64"
        description: "Model confidence score"

    exampleInput:
      user_id: 12345
      transaction_amount: 149.99
      merchant_category: "retail"
      features: "[0.5, 0.3, ..., 0.8]"

    exampleOutput:
      prediction: 0
      probability: 0.12
      confidence_score: 0.88

  # ===========================================================================
  # HYPERPARAMETERS
  # ===========================================================================
  hyperparameters:
    learning_rate: 0.001
    batch_size: 32
    epochs: 100
    max_depth: 6
    n_estimators: 200
    min_child_weight: 1
    subsample: 0.8
    colsample_bytree: 0.8
    gamma: 0.1
    reg_alpha: 0.01
    reg_lambda: 1.0
    early_stopping_rounds: 10

  # ===========================================================================
  # PERFORMANCE METRICS
  # ===========================================================================
  metrics:
    training:
      accuracy: 0.912
      precision: 0.887
      recall: 0.854
      f1_score: 0.870
      auc_roc: 0.945
      log_loss: 0.267

    validation:
      accuracy: 0.876
      precision: 0.852
      recall: 0.831
      f1_score: 0.841
      auc_roc: 0.912
      log_loss: 0.342

    test:
      accuracy: 0.869
      precision: 0.845
      recall: 0.823
      f1_score: 0.834
      auc_roc: 0.908
      log_loss: 0.356

    customMetrics:
      - name: "demographic_parity_difference"
        value: 0.014
        threshold: 0.05
        status: "PASS"
      - name: "equalized_odds_difference"
        value: 0.010
        threshold: 0.05
        status: "PASS"

  # ===========================================================================
  # STAGE HISTORY & TRANSITIONS
  # ===========================================================================
  stageHistory:
    - stage: "development"
      enteredAt: "2025-01-15T10:30:00Z"
      exitedAt: "2025-01-18T16:00:00Z"
      durationDays: 3

    - stage: "staging"
      enteredAt: "2025-01-18T16:00:00Z"
      exitedAt: "2025-01-22T10:00:00Z"
      durationDays: 4
      approvedBy: "jane.smith@example.com"
      approvalDate: "2025-01-18T15:45:00Z"
      approvalNotes: "Staging deployment approved after validation"

    - stage: "production"
      enteredAt: "2025-01-22T10:00:00Z"
      exitedAt: null  # Currently in production
      durationDays: null
      approvedBy: "john.doe@example.com"
      approvalDate: "2025-01-22T09:30:00Z"
      approvalNotes: "Production deployment approved by MRM committee"
      approvalCommittee: "Model Risk Management Committee"

  # ===========================================================================
  # MODEL ALIASES
  # ===========================================================================
  aliases:
    - alias: "champion"
      description: "Current production model serving majority of traffic"
      assignedAt: "2025-01-22T10:00:00Z"
      assignedBy: "mlops-team@example.com"

    - alias: "challenger"
      description: "Candidate model for A/B testing (10% traffic)"
      assignedAt: null
      assignedBy: null

    - alias: "canary"
      description: "Model for gradual rollout testing"
      assignedAt: null
      assignedBy: null

  # ===========================================================================
  # DEPLOYMENT INFORMATION
  # ===========================================================================
  deployment:
    endpoints:
      - name: "fraud-detection-prod"
        url: "https://api.example.com/v1/fraud-detection"
        platform: "[kubernetes | sagemaker | cloudrun | azureml]"
        region: "us-west-2"
        instanceType: "g4dn.xlarge"
        instanceCount: 3
        autoscaling:
          enabled: true
          minInstances: 2
          maxInstances: 10
          targetUtilization: 70
        deployedAt: "2025-01-22T10:15:00Z"

    trafficAllocation:
      - modelVersion: "1.0.0"
        alias: "champion"
        percentage: 90
      - modelVersion: "0.9.5"
        alias: "stable"
        percentage: 10

    rolloutStrategy: "[blue_green | canary | rolling | shadow]"
    rollbackPlan: "Automated rollback if error rate >1% or latency >100ms p99"

  # ===========================================================================
  # VALIDATION & TESTING
  # ===========================================================================
  validation:
    validationStatus: "[validated | pending | failed]"
    validatedBy: "model-validation-team@example.com"
    validationDate: "2025-01-20T14:00:00Z"
    validationReport: "https://docs.example.com/validation/model-v1.0.0-report.pdf"

    backTesting:
      conducted: true
      testPeriod:
        start: "2024-10-01"
        end: "2024-12-31"
      results:
        mape: 0.08  # Mean Absolute Percentage Error
        rmse: 0.12  # Root Mean Square Error
        status: "PASS"

    stressTesting:
      conducted: true
      scenarios:
        - scenario: "extreme_values"
          status: "PASS"
          notes: "Model stable under 3-sigma outliers"
        - scenario: "missing_features"
          status: "PASS"
          notes: "Graceful degradation with <10% features missing"

    biasAssessment:
      conducted: true
      assessmentDate: "2025-01-19T10:00:00Z"
      fairnessStatus: "PASS"
      report: "https://docs.example.com/bias/model-v1.0.0-bias-report.pdf"

  # ===========================================================================
  # GOVERNANCE & COMPLIANCE
  # ===========================================================================
  governance:
    riskTier: "[high | medium | low]"
    riskAssessment: "https://docs.example.com/risk/model-v1.0.0-assessment.pdf"

    approvals:
      - approver: "Jane Smith"
        role: "ML Lead"
        stage: "staging"
        approvedAt: "2025-01-18T15:45:00Z"
        decision: "approved"
      - approver: "John Doe"
        role: "Model Risk Manager"
        stage: "production"
        approvedAt: "2025-01-22T09:30:00Z"
        decision: "approved"
        conditions:
          - "Weekly performance monitoring required"
          - "Quarterly bias audits mandatory"

    compliance:
      regulations:
        - name: "GDPR Article 22"
          compliant: true
          evidence: "Explainability provided via SHAP"
        - name: "CCPA"
          compliant: true
          evidence: "User data handling compliant"
        - name: "EU AI Act"
          riskLevel: "limited_risk"
          compliant: true

    documentation:
      modelCard: "https://docs.example.com/models/model-name-card.md"
      trainingDataCard: "https://docs.example.com/data/dataset-card.md"
      technicalDocs: "https://docs.example.com/models/model-name-tech-docs.md"

  # ===========================================================================
  # MONITORING & OBSERVABILITY
  # ===========================================================================
  monitoring:
    dashboardUrl: "https://grafana.example.com/d/model-name"

    performanceMonitoring:
      enabled: true
      frequency: "hourly"
      metrics:
        - "prediction_accuracy"
        - "inference_latency"
        - "prediction_distribution"
      alerts:
        - metric: "accuracy"
          threshold: "<0.85"
          action: "page_on_call"
        - metric: "latency_p99"
          threshold: ">100ms"
          action: "alert_team"

    dataDriftMonitoring:
      enabled: true
      frequency: "daily"
      method: "kl_divergence"
      thresholds:
        kl_divergence: 0.10
      alerts:
        - condition: "kl_divergence > 0.15"
          action: "trigger_retraining_pipeline"

    fairnessMonitoring:
      enabled: true
      frequency: "weekly"
      metrics:
        - "demographic_parity"
        - "equalized_odds"
      thresholds:
        demographic_parity_diff: 0.08
        equalized_odds_diff: 0.08

  # ===========================================================================
  # RETRAINING & LIFECYCLE
  # ===========================================================================
  lifecycle:
    retrainingSchedule: "quarterly"
    retrainingTriggers:
      - "performance_drop_below_threshold"
      - "data_drift_detected"
      - "fairness_violation"
      - "scheduled_quarterly_refresh"

    lastRetrained: "2025-01-15T10:30:00Z"
    nextRetrainingDue: "2025-04-15T00:00:00Z"

    deprecationPlan:
      deprecated: false
      deprecationDate: null
      replacedBy: null
      eolDate: null

    versionHistory:
      - version: "0.9.5"
        deployedAt: "2024-10-01T10:00:00Z"
        retiredAt: "2025-01-22T10:00:00Z"
        status: "archived"
        notes: "Replaced by v1.0.0 with improved fairness"

  # ===========================================================================
  # INTEGRATION & DEPENDENCIES
  # ===========================================================================
  integrations:
    featureStore:
      enabled: true
      platform: "feast"
      featureService: "user-features-v2"

    monitoringTools:
      - tool: "evidently_ai"
        dashboardUrl: "https://monitoring.example.com/evidently/model-name"
      - tool: "fiddler"
        projectId: "project-123"

    cicd:
      pipelineUrl: "https://github.com/org/repo/actions/workflows/deploy-model.yml"
      automatedDeployment: true
      approvalRequired: true

    webhooks:
      - event: "stage_transition"
        url: "https://api.example.com/webhooks/model-stage-change"
        enabled: true
      - event: "performance_alert"
        url: "https://api.example.com/webhooks/model-alert"
        enabled: true

  # ===========================================================================
  # METADATA & AUDIT
  # ===========================================================================
  audit:
    accessLogs: "https://audit.example.com/models/model-name/access"
    changeHistory: "https://audit.example.com/models/model-name/changes"

    lastAccessedBy: "data-science-team@example.com"
    lastAccessedAt: "2025-01-26T09:15:00Z"

    totalDownloads: 45
    totalPredictions: 15234567

  contact:
    owner: "ml-team@example.com"
    technicalLead: "jane.smith@example.com"
    responsibleAI: "alice.johnson@example.com"

# ===========================================================================
# REGISTRY-SPECIFIC EXTENSIONS
# ===========================================================================

# MLflow-specific fields
mlflowExtensions:
  modelUri: "models:/model-name/1"
  runLink: "/#/experiments/123/runs/abc"
  registeredModelLink: "/#/models/model-name"

# SageMaker-specific fields
sagemakerExtensions:
  modelPackageArn: "arn:aws:sagemaker:us-west-2:123456789:model-package/model-name-v1"
  approvalStatus: "Approved"

# Azure ML-specific fields
azureMLExtensions:
  workspaceName: "ml-workspace-prod"
  modelPath: "azureml://models/model-name/versions/1"

# Vertex AI-specific fields
vertexAIExtensions:
  modelResourceName: "projects/123/locations/us-central1/models/model-name@1"
  endpoint: "projects/123/locations/us-central1/endpoints/456"
